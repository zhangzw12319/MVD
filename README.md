# MVD: Multi-view Variational Distillation for Person Re-identification

<!-- TOC -->

## Model Description

Mindspore implementation for **Farewell to Mutual Information: Variational Distillation for Cross-Modal Person Re-identification** in CVPR 2021. Please read [our paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Tian_Farewell_to_Mutual_Information_Variational_Distillation_for_Cross-Modal_Person_Re-Identification_CVPR_2021_paper.pdf) for a more detailed description of the training procedure. You can also read [Pytorch Version](https://github.com/FutabaSakuraXD/Farewell-to-Mutual-Information-Variational-Distiilation-for-Cross-Modal-Person-Re-identification) for further reference.

## Dataset

### Preparation

(1) SYSU-MM01 Dataset [1]: The SYSU-MM01 dataset can be downloaded from this [website](http://isee.sysu.edu.cn/project/RGBIRReID.htm).

**run `python pre_process_sysu.py`(in `DDAG_mindspore/third_party/pre_process_sysu.py`) in to prepare the dataset, the training data will be stored in ".npy" format.**

(2) RegDB Dataset [2]: The RegDB dataset can be downloaded from this [website](http://dm.dongguk.edu/link.html) by submitting a copyright form.

(Named: "Dongguk Body-based Person Recognition Database (DBPerson-Recog-DB1)" on their website).

If you have problems for acquiring data, please contact: zhangzw12319@163.com.

### Recommended Dataset Organization(Example)

```text
dataset
├──sysu                                     # SYSU-MM01 dataset
│  ├── cam1
│  ├── cam2
│  ├── cam3
│  ├── cam4
│  ├── cam5
│  ├── cam6
│  ├── exp
│  │   ├── available_id.mat
│  │   ├── available_id.txt
│  │   ├── test_id.mat
│  │   ├── test_id.txt
│  │   ├── train_id.mat
│  │   ├── train_id.txt
│  │   ├── val_id.mat
│  │   └── val_id.txt
│  ├── train_ir_resized_img.npy             # The following .npy files are generated by pre_process_sysu.py
│  ├── train_ir_resized_label.npy
│  ├── train_rgb_resized_img.npy
│  └── train_rgb_resized_label.npy
└── regdb                                   # RegDB Dataset
   ├── idx
   ├── Thermal
   └── Visible

```

Then you can set `--data-path Your own path/dataset/sysu` or `--data-path Your own path/dataset/regdb` according to above dataset structure. It's OK if you want to customize your data path.  But please note that the important thing is to set path to the folder which directly contains `cam X` etc. directories and ensures `.npy` files also inside(for SYSU-MM01), or to the folder which directly contains `idx` etc. directories(for RegDB).

## Environment Requirements

* Hardware
   * Support Ascend and GPU environment.
* Framework
   * [Mindspore](https://www.mindspore.cn)
* Third Package
   * Python==3.7.5
   * Mindspore>=1.3.0(See [Installation](https://www.mindspore.cn/install/))
   * Cuda==10.1
   * psutil*==5.8.0
   * tqdm*==4.62.0

*Note: these third party package are not stricted a specific version. For more details, please see `requriements.txt`.

## Quick Start

For GPU:

```shell
cd MVD/scripts/ # please enter this path before sh XXX.sh, otherwise path errors :)
sh run_standalone_train_sysu_gpu.sh
```

or

For Ascend:

```shell
cd MVD/scripts/ # please enter this path before sh XXX.sh, otherwise path errors :)
sh run_standalone_train_sysu_ascend.sh
```

## Script Description

### Scripts and Sample Code

```text
MVD
├── scripts
│   ├── run_eval.sh
│   ├── run_standalone_train_regdb_i2v_ascend.sh
│   ├── run_standalone_train_regdb_i2v_gpu.sh
│   ├── run_standalone_train_regdb_v2i_ascend.sh
│   ├── run_standalone_train_regdb_v2i_gpu.sh
│   ├── run_standalone_train_sysu_all_ascend.sh
│   ├── run_standalone_train_sysu_all_gpu.sh
│   ├── run_standalone_train_sysu_indoor_ascend.sh
│   └── run_standalone_train_sysu_indoor_gpu.sh
├── src
│   ├── dataset.py
│   ├── evalfunc.py
│   ├── loss.py
│   ├── models
│   │   ├── mvd.py
│   │   ├── resnet.py
│   │   ├── trainingcell.py
│   │   └── vib.py
│   └── utils.py
├── third_party
│   └── pre_process_sysu.py
├── train.py
├── eval.py
├── requirements.txt
└── README.md
```

### Script Parameters(Example)

```shell
# run)standalone_train_sysu_all_gpu.sh as an example
myfile="run_standalone_train_sysu_all_gpu.sh"

if [ ! -f "$myfile" ]; then
    echo "Please first enter MVD/scripts/run_standalone_train and run. Exit..."
    exit 0
fi

cd ..

# Note: --pretrain, --data-path arguments support global path or relative path(starting
#       from project root directory, i.e. /.../DDAG_mindspore/)

python train.py \
--MSmode PYNATIVE_MODE \
--dataset SYSU \
--data-path "Define your own path/sysu/" \
--optim adam \
--lr 0.0035 \
--device-target GPU \
--gpu 0 \
--pretrain "resnet50.ckpt" \
--tag "sysu_all" \
--loss-func id+tri \
--sysu-mode all \
--epoch 80 \
--print-per-step 100
```

The following table describes the most commonly used arguments. You can change freely as you want.

| Config Arguments  |                         Explanation                          |
| :---------------: | :----------------------------------------------------------: |
|    `--MSmode`     | Mindspore running mode, either 'GRAPH_MODE' or 'PYNATIVE_MODE'. |
| `--device-target` |              choose "GPU", "Ascend" or "Cloud"               |
|    `--dataset`    |              which dataset, "SYSU" or "RegDB".               |
|      `--gpu`      | which gpu to run(default: 0), only effective when `--device-target GPU` |
|   `--device-id`   | which Ascend AI core to run(default:0), only effective when `--device-target Ascend` |
|   `--data-path`   | manually define the data path(for `SYSU`, path folder must contain `.npy` files, see [`pre_process_sysu.py`](#anchor1) ). |
|   `--pretrain`    | specify resnet-50 pretrain file path(default "" for no ckpt file)* |
|    `--resume`     | specify checkpoint file path for whole model(default "" for no ckpt file, `--resume` loads weights after `--pretrain`, and thus will overwrite `--pretrain` weights)* |
|   `--sysu-mode`   | choose from `["all", "indoor"]`, only effective when `args.dataset=SYSU` |
|  `--regdb-mode`   | choose from `["i2v", "v2i"]`, only effective when `args.dataset=RegDB` |
|  `--save-period`  | specify  every XXX epochs to save network weights into checkpoint files |

***Note: Please note that mindspore compulsorily requires checkpoint files have `.cpkt` as file suffix, otherwise may trigger errors during loading.**

We recommend that these following hyper-parameters in `.sh` files should be kept by default. If you want ablation study or fine-tuning, feel free to change :)

| Config Arguments |                         Explanation                          |
| :--------------: | :----------------------------------------------------------: |
|    `--optim`     |             choose "adam" or "sgd"(default adam)             |
|      `--lr`      |     initial learning rate( 0.0035 for adam, 0.1 for sgd)     |
|     `--tags`     | (optional, but strongly recommended): You can name(tag) your experiments to organize you logs file, e.g. specifying `--tag Exp_1` will put your log files into "logs/Exp_1/XXX/".Default value is "toy", which means toy experiments(e.g. debugging logs). Fore more information, see [log organization](#log) |
|    `--epoch`     | the total number of training epochs, by default 80 Epochs(may be different from original paper). |
| `--warmup-steps` |                warm-up strategy, by default 5                |
| `--start-decay`  |         the start epoch of lr decay, by default 15.          |
|  `--end-decay`   |        the ending epoch of lr decay , by default 27.         |
|  `--loss-func`   | for ablation study, by default "id+tri" which is cross-entropy loss plus triplet loss. You can choose from `["id", "id+tri"]`. |

For more detailed and comprehensive arguments description, please refer to `train.py`.

## Training

### Training Process

For GPU:

```shell
cd MVD/scripts/ # please enter this path before sh XXX.sh, otherwise path errors :)
sh run_standalone_train_sysu_all_gpu.sh
```

For Ascend：

```shell
cd MVD/scripts/ # please enter this path before sh XXX.sh, otherwise path errors :)
sh run_standalone_train_sysu_all_ascend.sh
```

You can replace `run_standalone_train_sysu_all_gpu.sh` or `run_standalone_train_sysu_all_ascend.sh` with other training scripts.

### Transfer Training(Optional)

`--pretrain` parameter allows you to specify resnet50 checkpoint for pretraining, while `--resume` parameter allows you to specify to previously saved network checkpoints to resume training.

We use checkpoint from resnet50 pretrained on ImageNet, and the file [link](https://download.mindspore.cn/model_zoo/r1.1/resnet50_ascend_v111_imagenet2012_official_cv_bs32_acc76/).

For convenience, you can rename it `resnet50.ckpt` and save it directly under `MVD/`, then you can leave `--pretrain resnet50.ckpt` unchanged for convenience.

### Training Result

Training checkpoint will be stored in `logs/args.tag/training`, in which args.tag is specified before. The log <a id="log">organization</a> is like following.  It will be generated when you run `.sh` script and start training.

```text
logs
├── regdb_i2v
│   └── training
│   │   ├── epoch_60_rank1_65.51_mAP_62.42_RegDB_batch-size_2*8*4=64_adam_lr_0.0035_loss-func_id+tri_trial_1_main.ckpt
│   │   └── RegDB_batch-size_2*8*4=64_adam_lr_0.0035_loss-func_id+tri_trial_1_main_performance_2021-12-19_10-26-03.txt
│   └── testing
│       ├── ...
...
```

In `training` subdirectory,  `.txt` files are log files and `.ckpt` files are checkpoint files.

You will get result from training log file like the following:

```text
==> Start Training...
Epoch: [1][100/696]   Convolution LR: 0.0007000   IB & Classifier LR: 0.0000700    Loss:21.7580   id:9.3309   tri:12.4270   Batch Time:1.19  Accuracy:0.40
Epoch: [1][200/696]   Convolution LR: 0.0007000   IB & Classifier LR: 0.0000700    Loss:19.3218   id:9.1066   tri:10.2152   Batch Time:1.17  Accuracy:0.63
Epoch: [1][300/696]   Convolution LR: 0.0007000   IB & Classifier LR: 0.0000700    Loss:18.7310   id:8.8001   tri:9.9309   Batch Time:1.16  Accuracy:1.03
Epoch: [1][400/696]   Convolution LR: 0.0007000   IB & Classifier LR: 0.0000700    Loss:16.9942   id:8.4224   tri:8.5718   Batch Time:1.16  Accuracy:1.55
Epoch: [1][500/696]   Convolution LR: 0.0007000   IB & Classifier LR: 0.0000700    Loss:16.3901   id:8.1935   tri:8.1966   Batch Time:1.15  Accuracy:2.10
Epoch: [1][600/696]   Convolution LR: 0.0007000   IB & Classifier LR: 0.0000700    Loss:15.1044   id:7.8598   tri:7.2447   Batch Time:1.15  Accuracy:2.74
...
```

At the end of every epoch training, `train.py` will use a random testing set (different from training set) to evaluate the model performance. So you will see rank-1 and mAP performance. And this programming pattern of evaluation is analogy to `test.py`.

## Evaluation

### Evaluation Process

For GPU：

```shell
cd MVD/scripts/ # please enter this path before sh XXX.sh, otherwise path errors :)
sh run_eval_sysu_all_gpu.sh
```

For Ascend:

```shell
cd DDAG_mindspore/scripts/ # please enter this path before sh XXX.sh, otherwise path errors :)
sh run_eval_sysu_all_ascend.sh
```

### Evaluation Result

Before you start testing, please set `--resume` in `run_eval_XXX.sh` . You can find your saved checkpoints in corresponding `/logs/args.tag/training/` path.  After running `run_eval_XXX.sh`, you will get result from testing log file like the following:

```text
Resume checkpoint:/.../logs/sysu_all_part_graph/training/epoch_25_rank1_57.04_mAP_55.76_SYSU_batch-size_2*8*4=64_adam_lr_0.0035_loss-func_id+tri_P_3_Graph__main.ckpt
Start epoch: 25
For SYSU-MM01 all search, the testing result is:
FC:   Rank-1: 56.96% | Rank-5: 82.83% | Rank-10: 91.04%| Rank-20: 96.04%| mAP: 55.28%
FC_att:   Rank-1: 57.42% | Rank-5: 82.59% | Rank-10: 90.91%| Rank-20: 96.17%| mAP: 55.33%
******************************************************************************
```

## Performance

### Training Performance

| Parameters                 | Ascend 910                                                   | GPU(RTX Titan) |
| -------------------------- | ------------------------------------------------------------ | ----------------------------------------------|
| Model Version              | MVD: baseline + modal specific & modal share backbone + VIB | MVD: baseline + modal specific & modal share backbone + VIB |
| Resource                   | Ascend 910; CPU 2.60GHz, 192cores; Memory 755G; OS Euler2.8  |  NVIDIA RTX Titan-24G        |
| uploaded Date              | 12/19/2021 (month/day/year)                 | 12/19/2021 (month/day/year)          |
| MindSpore Version          | 1.3.0, 1.5.0                                             | 1.3.0, 1.5.0                                  |
| Dataset                    | SYSU-MM01, RegDB                              | SYSU-MM01, RegDB           |
| Training Parameters（SYSU-MM01） | Epochs=80, steps per epoch=695, batch_size = 64 | epoch=80, steps per epoch=64 batch_size = 64 |
| Training Parameters（RegDB） | Epochs=80, steps per epoch=695, batch_size = 64 | epoch=80, steps per epoch=64 batch_size = 64 |
| Optimizer                  | Adam                                                 | Adam                                  |
| Loss Function              | Softmax Cross Entropy + Triplet Loss                         | Softmax Cross Entropy + Triplet Loss          |
| outputs                    | feature vector + probability                              | feature vector + probability               |
| Loss                       | 1.7161                                     |  2.0663      |
| Speed                      | 830 ms/step（1pcs, PyNative Mode）               | 940ms/step（1pcs, PyNative Mode） |
| Total time                 | SYSU: about  13h; RegDB: about 3h30min | SYSU: about  14h; RegDB: about 4hmin |
| Parameters (M)             | 161.9M                                     | 161.9M                        |
| Checkpoint for Fine tuning | 329.2M (.ckpt file)                                     | 329.2M (.ckpt file)                     |
| Scripts                    | [link](https://gitee.com/mindspore/models/research/cv/MVD) ||

### Inference Performance

| Parameters        | Ascend                                                      | GPU(RTX Titan)                                              |
| ----------------- | ----------------------------------------------------------- | ----------------------------------------------------------- |
| Model Version     | MVD: baseline + modal specific & modal share backbone + VIB | MVD: baseline + modal specific & modal share backbone + VIB |
| Resource          | Ascend 910; OS Euler2.8                                     | NVIDIA RTX Titan-24G                                        |
| Uploaded Date     | 12/19/2021 (month/day/year)                                 | 12/19/2021 (month/day/year)                                 |
| MindSpore Version | 1.5.0, 1.3.0                                                | 1.5.0, 1.3.0                                                |
| Dataset           | SYSU-MM01, RegDB                                            | SYSU-MM01, RegDB                                            |
| batch_size        | 64                                                          | 64                                                          |
| outputs           | feature                                                     | feature                                                     |
| Accuracy          | See following 4 tables ↓                                    |                                                             |

### SYSU-MM01 (all-search mode)

| Metric | Value(Pytorch) | Value(Mindspore, GPU) |
| :----: | :------------: | :-------------------: |
| Rank-1 |     60.02%     |        60.08%         |
|  mAP   |     58.80%     |        57.55%         |

### SYSU-MM01 (indoor-search mode)

| Metric | Value(Pytorch) | Value(Mindspore, GPU) |
| :----: | :------------: | :-------------------: |
| Rank-1 |     66.05%     |        69.57%         |
|  mAP   |     72.98%     |        73.13%         |

### RegDB(Visible-Thermal)

| Metric | Value(Pytorch) | Value(Mindspore, GPU, --trial 1) |
| :----: | :------------: | :------------------------------: |
| Rank-1 |     73.20%     |              76.50%              |
|  mAP   |     71.60%     |              72.35%              |

### RegDB(Thermal-Visible)

| Metric | Value(Pytorch) | Value(Mindspore, GPU, --trial 1) |
| :----: | :------------: | :------------------------------: |
| Rank-1 |     71.80%     |              77.91%              |
|  mAP   |     70.10%     |              71.37%              |

***Note**: The aforementioned pytorch results can be seen in original [pytorch repo](https://github.com/FutabaSakuraXD/Farewell-to-Mutual-Information-Variational-Distiilation-for-Cross-Modal-Person-Re-identification).

## Description of Random Situation

* In `utils.py`, `IdentitySampler` used to sample different identities and images in both visible and infrared(thermal) modal, and we set random seed in `IndentitySampler`. This randomness will affect both inference part in `train.py` and in `eval.py`. Therefore small different rank-1 and mAP fluctuations(about 1%) between inference in `train.py` and `eval.py` may be seen even on the same training results.

* When testing on RegDB dataset, there is a `--trial` argument specifying which id to be selected; different  `--trial`  choosing may cause slight rank-1 mAP fluctuation.

## Reference

Please kindly cite the original paper references in your publications if it helps your research:

```text
@inproceedings{VariationalDistillation,
title={Farewell to Mutual Information Variational Distiilation for Cross-Modal Person Re-identification},
author={Xudong Tian and Zhizhong Zhang and Shaohui Lin and Yanyun Qu and Yuan Xie and Lizhuang Ma},
booktitle={Computer Vision and Pattern Recognition},
year={2021}
}
```

Please kindly reference the url of mindspore repository in your code if it helps your research and code.
